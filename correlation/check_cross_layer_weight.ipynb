{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiV_OVyOy8vL",
        "outputId": "88165eaf-9c73-4556-fcc8-7bf8180beb32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/96.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m92.2/96.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "d873gYbD0CGF"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import tensorflow_recommenders as tfrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8oRrlXZI0ETP"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import sys\n",
        "import csv\n",
        "import gzip\n",
        "import copy\n",
        "import datetime\n",
        "import pickle\n",
        "from sklearn import metrics\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tIqMLfCl0K2k"
      },
      "outputs": [],
      "source": [
        "seed_value = 42  # seed for reproducibility\n",
        "random.seed(seed_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mtzzM19O0ZKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c01b0c4-d08d-419f-c286-bb4df86027f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "l_8Fpg8C9LZS"
      },
      "outputs": [],
      "source": [
        "sys.path.append('/content/drive/MyDrive/ctr/code/model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DEaL_7Vy9YD2"
      },
      "outputs": [],
      "source": [
        "import dcn\n",
        "import run_models\n",
        "import run_models_save_model\n",
        "import check_cross_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "PR_SHlAlw3NJ"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 100000\n",
        "SHUFFLE_BUFFER_SIZE = 10000\n",
        "SHUFFLE_SEED = 42\n",
        "DATASET_EPOCHS = 1\n",
        "FITTING_EPOCHS = 1\n",
        "LEARNING_RATE = 0.0001\n",
        "DCN_PARALLEL = True\n",
        "EMBEDDING_DIMENSION = 32\n",
        "STR_COLUMNS = [\n",
        "    'click', 'banner_pos',\n",
        "    'site_id', 'site_domain', 'site_category',\n",
        "    'app_id', 'app_domain', 'app_category',\n",
        "    'device_id', 'device_ip', 'device_model', 'device_type', 'device_conn_type',\n",
        "    'C1', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21',\n",
        "    'day_of_week'\n",
        "    ]\n",
        "INT_COLUMNS = [\n",
        "    'hour_of_day'\n",
        "]\n",
        "COLUMN_DEFAULTS = [tf.string] * (len(STR_COLUMNS) -1) + [tf.float32] * len(INT_COLUMNS) + [tf.string] * 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gIoZoTFOSxbt"
      },
      "outputs": [],
      "source": [
        "# column_defaults를 원본 CSV File의 Column 순서로 인식한다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AMmCSs0MxEBN"
      },
      "outputs": [],
      "source": [
        "TRAIN_FILE = '/content/drive/MyDrive/ctr/avazu/processed/train/train.csv'\n",
        "train_batches = tf.data.experimental.make_csv_dataset(\n",
        "    TRAIN_FILE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    select_columns= STR_COLUMNS + INT_COLUMNS,\n",
        "    column_defaults=COLUMN_DEFAULTS,\n",
        "    shuffle=True, shuffle_buffer_size=SHUFFLE_BUFFER_SIZE, shuffle_seed=SHUFFLE_SEED,\n",
        "    num_epochs=DATASET_EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JV6KkFW_7TMC"
      },
      "outputs": [],
      "source": [
        "TEST_FILE = '/content/drive/MyDrive/ctr/avazu/processed/train/test.csv'\n",
        "test = tf.data.experimental.make_csv_dataset(\n",
        "    TEST_FILE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    select_columns= STR_COLUMNS + INT_COLUMNS,\n",
        "    column_defaults=COLUMN_DEFAULTS,\n",
        "    num_epochs=DATASET_EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Y6RrztcX7q25"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/ctr/avazu/processed/train/vocabularies.p', 'rb') as f:\n",
        "    total_voca = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VoLF_VKb7_pj"
      },
      "outputs": [],
      "source": [
        "FEATURE_ENGINEERING = {\n",
        "    'embedding' : [\n",
        "        'site_id', 'site_domain', 'site_category',\n",
        "        'app_id', 'app_domain', 'app_category',\n",
        "        'device_model', 'device_type',\n",
        "        'C1', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21',\n",
        "        'day_of_week'\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "f8LTV5xoAMOn"
      },
      "outputs": [],
      "source": [
        "vocabularies = {}\n",
        "\n",
        "vocabularies['embedding'] = {}\n",
        "for feature in FEATURE_ENGINEERING['embedding']:\n",
        "    vocabularies['embedding'][feature] = total_voca[feature]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "H7w75Z08GPlO"
      },
      "outputs": [],
      "source": [
        "str_features = list(vocabularies['embedding'].keys())\n",
        "int_features = ['hour_of_day']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Osk4pWksIncf"
      },
      "outputs": [],
      "source": [
        "nodes = [32, 64, 128, 256, 512, 1024]\n",
        "deep_layers = [1, 2, 3, 4, 5]\n",
        "cross_layers = [1, 2, 3, 4, 5, 6]\n",
        "result_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Y264Xnlke8BW"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "27xdQUl0fwpM"
      },
      "outputs": [],
      "source": [
        "for engineering_method, voca_set in vocabularies.items():\n",
        "    for feature, voca in voca_set.items():\n",
        "        vocabularies[engineering_method][feature] = list(map(str, voca))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "E6RaTOb03hc8"
      },
      "outputs": [],
      "source": [
        "HISTORY_FILE_DIR = '/content/drive/MyDrive/ctr/data/baseline/'\n",
        "HISTORY_FILE_NAME = 'fitting_history.V1.3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xqYdN7404DJW"
      },
      "outputs": [],
      "source": [
        "CHECKPOINTS_DIR = '/content/drive/MyDrive/ctr/data/baseline/'\n",
        "CHECKPOINTS_NAME = 'checkpoints.V1.3'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "node, deep_layer, cross_layer = 1024, 4, 1\n",
        "\n",
        "model = dcn.DCN(\n",
        "    dcn_parallel=DCN_PARALLEL,\n",
        "    cross_layer_size=cross_layer,\n",
        "    deep_layer_sizes=[node]*deep_layer,\n",
        "    vocabularies=vocabularies,\n",
        "    str_features=str_features,\n",
        "    int_features=int_features,\n",
        "    embedding_dimension=EMBEDDING_DIMENSION)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE))"
      ],
      "metadata": {
        "id": "tHq74XVEErsu"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fitting_data = {\n",
        "    'Train LogLoss' : [],\n",
        "    'Train AUC' : [],\n",
        "    'Test LogLoss' : [],\n",
        "    'Test AUC' : []\n",
        "}\n",
        "EPOCHS = 50\n",
        "for epoch in range(EPOCHS):\n",
        "    history = model.fit(train_batches, epochs=1, verbose=True)\n",
        "    metrics = model.evaluate(test, return_dict=True)\n",
        "    fitting_data['Train LogLoss'].append(history.history[\"LogLoss\"][0])\n",
        "    fitting_data['Train AUC'].append(history.history[\"AUC\"][0])\n",
        "    fitting_data['Test LogLoss'].append(metrics[\"LogLoss\"])\n",
        "    fitting_data['Test AUC'].append(metrics[\"AUC\"])\n",
        "    if (epoch % 10) == 0:\n",
        "        print(\"{}th Epoch\".format(epoch+1))\n",
        "        with open(HISTORY_FILE_DIR + HISTORY_FILE_NAME + '.p', 'wb') as f:\n",
        "            pickle.dump(fitting_data, f)\n",
        "        model.save_weights(CHECKPOINTS_DIR + CHECKPOINTS_NAME + '.weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx0K8SImPyh0",
        "outputId": "25062f65-d2b3-482c-9782-8a5c439ff206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     25/Unknown \u001b[1m76s\u001b[0m 3s/step - AUC: 0.5467 - LogLoss: 0.4739 - loss: 0.4626 - regularization_loss: 0.0000e+00 - total_loss: 0.4626"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mat = model._cross_layers[0].weights[0]\n",
        "features = FEATURE_ENGINEERING['embedding']"
      ],
      "metadata": {
        "id": "DLnzQOT8YVIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.axes_grid1 import make_axes_locatable"
      ],
      "metadata": {
        "id": "_pfW19Hv8YsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_norm = np.ones([len(features), len(features)])\n",
        "ticks = list(range(len(features)))\n",
        "dim = model.embedding_dimension\n",
        "\n",
        "# Compute the norms of the blocks.\n",
        "for i in range(len(features)):\n",
        "  for j in range(len(features)):\n",
        "    block = mat[i * dim:(i + 1) * dim,\n",
        "                j * dim:(j + 1) * dim]\n",
        "    block_norm[i,j] = np.linalg.norm(block, ord=\"fro\")\n",
        "\n",
        "plt.figure(figsize=(200,200))\n",
        "im = plt.matshow(block_norm, cmap=plt.cm.Blues)\n",
        "ax = plt.gca()\n",
        "\n",
        "divider = make_axes_locatable(plt.gca())\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "plt.colorbar(im, cax=cax)\n",
        "cax.tick_params(labelsize=10)\n",
        "\n",
        "ax.set_xticks(range(len(features)))\n",
        "ax.set_yticks(range(len(features)))\n",
        "\n",
        "_ = ax.set_xticklabels(features, rotation=45, ha=\"left\", fontsize=10)\n",
        "_ = ax.set_yticklabels(features, fontsize=10)\n",
        "\n",
        "plt.savefig('/content/drive/MyDrive/ctr/data/baseline/weight_analysis.png')"
      ],
      "metadata": {
        "id": "wFRWG3Jm9tPp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}